{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ligandome database export guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not distribute external databases in this repo - instead, we provide this notebook as a guide for users to export publically available datasets and databases to run the ligandome workflow. **Some parts of this notebook require visiting sites and manually searching**. Please find the licenses for each data source under `./licenses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLELES =  [\n",
    "            'A0101',\n",
    "            'A0201',\n",
    "            'A0207',\n",
    "            'A0301',\n",
    "            'A1101',\n",
    "            'A2301',\n",
    "            'A2402',\n",
    "            'A3001',\n",
    "            'A3303',\n",
    "            'B0702',\n",
    "            'B0801',\n",
    "            'B1501',\n",
    "            'B1502',\n",
    "            'B1503',\n",
    "            'B3501',\n",
    "            'B4001',\n",
    "            'B4006',\n",
    "            'B4402',\n",
    "            'B4403',\n",
    "            'B4601',\n",
    "            'B5101',\n",
    "            'B5201',\n",
    "            'B5301',\n",
    "            'B5801',\n",
    "            'C0102',\n",
    "            'C0202',\n",
    "            'C0304',\n",
    "            'C0401',\n",
    "            'C0602',\n",
    "            'C0701',\n",
    "            'C0702',\n",
    "            'C0801',\n",
    "            'C1502',\n",
    "            'C1601'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_DATA_DIR = Path('./tmp_data')\n",
    "DATABASE_EXPORTS_PATH = Path('../ligandome/database_exports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_DATA_DIR.mkdir(exist_ok=True)\n",
    "DATABASE_EXPORTS_PATH.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. HLA Ligand ATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HLA_TMP_DATA_DIR = TMP_DATA_DIR / 'HLA_LIGAND_ATLAS'\n",
    "HLA_TMP_DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HLA_LIGAND_ATLAS_ALLELES = [allele for allele in ALLELES if allele not in [\n",
    "    'B5301',\n",
    "    'B1503',\n",
    "    'A0207',\n",
    "    'C1502',\n",
    "    'B5201',\n",
    "    'C0801',\n",
    "    'B5101',\n",
    "    'B4601',\n",
    "    'A3303',\n",
    "    'B4006',\n",
    "    'B4001',\n",
    "    'C0102',\n",
    "    'B1502',\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a small issue with the export function on HLA Ligand ATLAS's main site, we use a loop here to query and grab data for alleles of interest one by one, then aggregate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, output_file: Path, extract: bool=False) -> None:\n",
    "    file_grab = requests.get(url)\n",
    "    assert file_grab.status_code == 200, f'Problem downloading file from {url}'\n",
    "    with open(output_file, 'wb+') as outfile:\n",
    "        outfile.write(file_grab.content)\n",
    "    if extract:\n",
    "        with zipfile.ZipFile(output_file,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(output_file.parent / output_file.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_results = Parallel(4)(delayed(download_file)(f'https://hla-ligand-atlas.org/peptides/download?&h=sw/{allele[0]}*{allele[1:3]}:{allele[3:]}', HLA_TMP_DATA_DIR / f'HLA_{allele}.zip', True) for allele in tqdm(HLA_LIGAND_ATLAS_ALLELES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for allele in tqdm(HLA_LIGAND_ATLAS_ALLELES):\n",
    "    df = pd.read_csv(HLA_TMP_DATA_DIR / f'HLA_{allele}' / 'hla_ligand_atlas' / 'HLA_aggregated.tsv', sep='\\t')\n",
    "    df['allele'] = allele\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hla = pd.concat(dfs)\n",
    "final_hla.to_csv(DATABASE_EXPORTS_PATH / 'HLA_ligand_atlas_aggregated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IEDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEDB_TMP_DATA_DIR = TMP_DATA_DIR / 'IEDB'\n",
    "IEDB_TMP_DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export multiple subsets of IEDB to ensure we can standardise the source of each peptide. This section requires you manually visit the [IEDB site here](https://www.iedb.org/home_v3.php) and download several files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Healthy peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEDB_HEALTHY_DATA_DIR = IEDB_TMP_DATA_DIR / 'Healthy'\n",
    "IEDB_HEALTHY_DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the following options in IEDB and click `Search`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](iedb_search_guide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then scroll to the `Disease` panel at the bottom and select `None (Healthy)`, then click `Search` again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](iedb_healthy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the `Export Results` button in the top right of the page, and export the data in the following format:\n",
    "- File Format: `.CSV`\n",
    "- Header Row Format: `Single Header`\n",
    "- Export Type: `Full, all data columns`\n",
    "\n",
    "And leave `Columns to Include` as the default value. Then hit `Export` and save the data as `tmp_data/IEDB/Healthy/IEDB_healthy_export.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(IEDB_HEALTHY_DATA_DIR / 'IEDB_healthy_export.csv')\n",
    "temp_df.to_csv(DATABASE_EXPORTS_PATH / 'IEDB_healthy_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tumour origin peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEDB_TUMOUR_DATA_DIR = IEDB_TMP_DATA_DIR / 'Tumour'\n",
    "IEDB_TUMOUR_DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the following options in IEDB and click `Search`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](iedb_search_guide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then scroll to the `Disease` panel at the bottom and select `Cancer`, then click `Search` again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](iedb_cancer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the `Export Results` button in the top right of the page, and export the data in the following format:\n",
    "- File Format: `.CSV`\n",
    "- Header Row Format: `Single Header`\n",
    "- Export Type: `Full, all data columns`\n",
    "\n",
    "And leave `Columns to Include` as the default value. Then hit `Export` and save the data as `tmp_data/IEDB/Tumour/IEDB_tumour_export.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(IEDB_TUMOUR_DATA_DIR / 'IEDB_tumour_export.csv')\n",
    "temp_df.to_csv(DATABASE_EXPORTS_PATH / 'IEDB_tumour_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Viral origin peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEDB_VIRAL_DATA_DIR = IEDB_TMP_DATA_DIR / 'Viral'\n",
    "IEDB_VIRAL_DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We additionally include a small amount of viral data from IEDB (which we later supplement with VDJDB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the following options in IEDB and click `Search`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](iedb_viral.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the `Export Results` button in the top right of the page, and export the data in the following format:\n",
    "- File Format: `.CSV`\n",
    "- Header Row Format: `Single Header`\n",
    "- Export Type: `Full, all data columns`\n",
    "\n",
    "And leave `Columns to Include` as the default value. Then hit `Export` and save the data as `tmp_data/IEDB/Viral/IEDB_viral_export.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(IEDB_VIRAL_DATA_DIR / 'IEDB_viral_export.csv')\n",
    "temp_df.to_csv(DATABASE_EXPORTS_PATH / 'IEDB_viral_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Assay mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally for IEDB we need the data to map the assay IDs back to labelled MHC I alleles. For this, please download and extract the following large file into `tmp_data/IEDB/mhc_ligand_full.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.iedb.org/downloader.php?file_name=doc/mhc_ligand_full_single_file.zip -O {IEDB_TMP_DATA_DIR}/mhc_ligand_full.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip {IEDB_TMP_DATA_DIR}/mhc_ligand_full.zip -d {IEDB_TMP_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then post-process this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(IEDB_TMP_DATA_DIR / 'mhc_ligand_full.csv', header=[0, 1])\n",
    "df.columns = [f'{i}_{j}' for i, j in df.columns]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Epitope IRI'] = df['Epitope_Epitope IRI'].apply(lambda x: x.split('/')[-1])\n",
    "df['mhc_allele'] = df['MHC Restriction_Name']\n",
    "df[['Epitope IRI','mhc_allele']].to_csv(DATABASE_EXPORTS_PATH / 'IEDB_ligand_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NetMHCPan 4.1 training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETMHCPAN_TMP_DATA_DIR = TMP_DATA_DIR / 'NETMHCPAN'\n",
    "NETMHCPAN_TMP_DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract training data archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_grab = requests.get('https://services.healthtech.dtu.dk/suppl/immunology/NAR_NetMHCpan_NetMHCIIpan/NetMHCpan_train.tar.gz')\n",
    "with open(NETMHCPAN_TMP_DATA_DIR / 'NetMHCpan_train.tar.gz', 'wb+') as archive:\n",
    "    archive.write(file_grab.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the archive (make sure `tar` is installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf {NETMHCPAN_TMP_DATA_DIR}/NetMHCpan_train.tar.gz -C {NETMHCPAN_TMP_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then concatenate all the monoallelic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_files = [f for f in (NETMHCPAN_TMP_DATA_DIR / 'NetMHCpan_train').iterdir() if '00' in f.stem]\n",
    "netmhcpan_all = pd.concat([pd.read_csv(df, names=['peptide','Target','AlleleTemp'], header=None, sep=' ') for df in training_data_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allele_keys = pd.read_csv(NETMHCPAN_TMP_DATA_DIR / 'NetMHCpan_train' / 'allelelist', sep='\\t| ', header=None, names=['Experiment','Allele List'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netmhcpan_all['allele'] = netmhcpan_all['AlleleTemp'].map(dict(zip(allele_keys['Experiment'], allele_keys['Allele List'])))\n",
    "netmhcpan_all['mono_allelic'] = (netmhcpan_all['allele'] == netmhcpan_all['AlleleTemp'])\n",
    "netmhcpan_monoallelic = netmhcpan_all.loc[netmhcpan_all['mono_allelic']]\n",
    "netmhcpan_monoallelic.to_csv(DATABASE_EXPORTS_PATH / 'NetMHCPan_monoallelic_training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. VDJDB viral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDJDB_TMP_DATA_DIR = TMP_DATA_DIR / 'VDJDB'\n",
    "VDJDB_TMP_DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there exists no simple way to export all VDJDB viral data; here we access the website and manually add all available viral species as search filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, visit the VDJDB database browser page - https://vdjdb.cdr3.net/search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the `MHC` heading, untick `MHCII`, then under the `Antigen` heading, add all the following viral origin options to the `Source species` filter (unfortunately this has to be done one by one):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](vdjdb_selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally hit `Export as:`, then `TSV`, saving the file as `tmp_data/VDJDB/VDJDB_viral.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(VDJDB_TMP_DATA_DIR / 'VDJDB_viral.tsv', sep='\\t')\n",
    "temp_df.to_csv(DATABASE_EXPORTS_PATH / 'VDJDB_viral.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. UniProt human proteome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we need a local copy of the human proteome, which we obtain from UniProt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_human_proteome_fasta(fasta_outfile: Path) -> None:\n",
    "    \"\"\"Fetch a fasta file of the entire human reference proteome.\n",
    "\n",
    "    Args:\n",
    "        fasta_outfile (Path): Path to save the fasta file.\n",
    "    \"\"\"\n",
    "    canonical_proteome = requests.get('https://rest.uniprot.org/uniprotkb/stream?format=fasta&query=%28reviewed%3Atrue%20AND%20proteome%3Aup000005640%29')\n",
    "\n",
    "    if not Path(fasta_outfile).exists():\n",
    "        with open(fasta_outfile, 'a+') as fasta:\n",
    "            fasta.write(canonical_proteome.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_human_proteome_fasta(DATABASE_EXPORTS_PATH / 'UniProtCanonicalProteome.fasta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ligandome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
